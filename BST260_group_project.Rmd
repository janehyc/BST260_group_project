---
title: "Group_project"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(broom)
library(olsrr)
library(ggplot2)
library(VGAM)
library(nnet)
library(DescTools)
library(ResourceSelection)
library(LogisticDx)
library(knitr)
library(mice)
library(caret)
library(leaps)
library(MASS)
library(glmnet)
library(splines)
library(splines2)
library(GGally)
```

## Forest Fire
```{r}
# Add season category 
fire <- read.csv("forestfires.csv", header=TRUE, sep = ",")
fire$season <- rep("spring", 517)
for (i in 1:517){
if (fire$month[i] %in% c("feb","jan","dec")) fire$season[i] <- "winter"
if (fire$month[i] %in% c("oct","nov","sep")) fire$season[i] <- "autumn"
if (fire$month[i] %in% c("aug","jul","jun")) fire$season[i] <- "summer"
}
fire$season <- as.factor(fire$season)

fire$season.cat <- rep(0, 517)
for (i in 1:517){
  if (fire$season[i] == "summer") {
    fire$season.cat[i] <- 1
  }
  if (fire$season[i] == "autumn") {
    fire$season.cat[i] <- 2
  }
  if (fire$season[i] =="winter") {
    fire$season.cat[i] <- 3
  }
}
summary(fire)
```



```{r}
# Area log transformation (for area>0)
hist(fire$area,40, main = "Histogram of area", xlab = "Area")
fire["logarea"] <- ifelse(fire$area >0, log(fire$area), NA)
ggplot(data=fire, aes(x=logarea))+
  geom_histogram(aes(y=..density..), col="black",fill="white")+
  stat_function(fun=dnorm, args = list(mean=mean(fire$logarea, na.rm = TRUE), sd = sd(fire$logarea, na.rm=TRUE)),col="red")
```




#### Outliers
```{r}
# Outliers / Cook's Distance
ols_plot_cooksd_bar(mod_lin)  #number 262 datapoint in this data set, which is id=500
ols_plot_resid_lev(mod_lin)
ols_plot_dffits(mod_lin)

cooksd <- cooks.distance(mod_lin)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd+0.001, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")

# Get rid off the outliers
influential = which(cooksd>4*mean(cooksd, na.rm=T))  # influential points
influ <- area_posit[influential, ]   #all data for influential points
influ
dat <- area_posit[-influential,]  #get rid off the influential points
head(dat)
```




#### Linear Regression Analysis
```{r}
# Scatterplot with lowess curve
ggplot(data=area_posit, aes(x=DMC, y=logarea))+
  geom_point()+
  geom_smooth(method = "loess",se=FALSE)+
  theme_bw()


# Fit linear regression model to area>0 since this is normally distributed
area_posit <- fire[which(fire$area>0),]
summary(area_posit)
area_posit

mod_lin <- lm(logarea ~X+Y+month+day+FFMC+DMC+DC+ISI+temp+RH+wind+rain, data=area_posit[c(seq(1,12),15,16)])
summary(mod_lin)  # DMC and DC are being statistically significant 
tidy(mod_lin)

#Fit linear regression again, now without the influential points and outliers
mod_lin2 <- lm(logarea ~X+Y+month+day+FFMC+DMC+DC+ISI+temp+RH+wind+rain, data=dat)
summary(mod_lin2)

plot(mod_lin2, which=c(1,2,3))
```





```{r}
# Linear model with forward selection & backward elimination based on AIC
full.model <- lm(logarea ~., data = dat[c(seq(1,12),15,16)])
step.forw <- step(lm(logarea~1, data=dat[c(seq(1,12),15,16)]), ~X+Y+month+day+FFMC+DMC+DC+ISI+temp+RH+wind+rain, direction = "forward")
step.back <- step(full.model, direction = "backward")
summary(step.forw)
summary(step.back)

plot(step.back, 1:3)



table1<- matrix(c(summary(mod_lin)$r.squared, summary(mod_lin)$adj.r.squared, sqrt(mean(mod_lin$residuals^2)),AIC(mod_lin), BIC(mod_lin),
                  summary(mod_lin2)$r.squared, summary(mod_lin2)$adj.r.squared, sqrt(mean(mod_lin2$residuals^2)),AIC(mod_lin2), BIC(mod_lin2),
                  summary(step.forw)$r.squared, summary(step.forw)$adj.r.squared, sqrt(mean(step.forw$residuals^2)),AIC(step.forw), BIC(step.forw),
                  summary(step.back)$r.squared, summary(step.back)$adj.r.squared, sqrt(mean(step.back$residuals^2)),AIC(step.back), BIC(step.back)), ncol=5,nrow=4, byrow=TRUE)
colnames(table1)<- c("R^2", "Adjusted R^2", "Square root of MSE","AIC", "BIC")
rownames(table1)<- c("logarea ~.",
                     "logarea~., (w/o influential points)",
                     "model with forward selection based on AIC",
                     "model with backward elimination based on AIC")
table1 <- as.table(table1)
kable(table1)
```

```{r}
table10<- matrix(c(round(summary(mod_lin2)$r.squared,5), round(summary(mod_lin2)$adj.r.squared,5), round(sqrt(mean(mod_lin2$residuals^2)),5),round(AIC(mod_lin2),5), round(BIC(mod_lin2),5),
                  round(summary(step.forw)$r.squared,5), round(summary(step.forw)$adj.r.squared,5), round(sqrt(mean(step.forw$residuals^2)),5),round(AIC(step.forw),5), round(BIC(step.forw),5),
                  round(summary(step.back)$r.squared,5), round(summary(step.back)$adj.r.squared,5), round(sqrt(mean(step.back$residuals^2)),5),round(AIC(step.back),5), round(BIC(step.back),5),
                  "/", "/", "/",round(AIC(mod.gam),5), round(BIC(mod.gam),5)), ncol=5,nrow=4, byrow=TRUE)
colnames(table10)<- c("R^2", "Adjusted R^2", "Square root of MSE","AIC", "BIC")
rownames(table10)<- c("logarea~., (w/o influential points)",
                     "model with forward selection based on AIC",
                     "model with backward elimination based on AIC",
                     "model fitted with gamma distribution")
table10 <- as.table(table10)
kable(table10)
```





### New complete dataset
```{r}
new_dat <- rbind(fire[which(fire$area==0),], dat)   #join the area_positive w/o influential points to the data w/ area=0
new_dat$burn <- ifelse(new_dat$area==0,0,1)  #to get the new dataset without the influential points
new_dat
md.pattern(new_dat)  #no missing value
summary(new_dat)
```





### Logistic Regression
```{r}
# Fit Logistic Regression
lg_burn <- glm(burn ~ X+Y+month+day+FFMC+DMC+DC+ISI+temp+RH+wind+rain, family=binomial(link = "logit"),data=new_dat)
summary(lg_burn)
lg_burn2 <- glm(burn ~X+Y+month+day+FFMC+I(FFMC^2)+DMC+I(DMC^2)+DC+I(DC^2)+ISI+temp+I(temp^2)+RH+wind+rain+I(rain^2), family=binomial(link = "logit"), data=new_dat)
summary(lg_burn2)

hoslem.test(new_dat$burn, fitted(lg_burn), g=10)  #not a poor fit (small P-value, poor fit, H0:good fit)
hoslem.test(new_dat$burn, fitted(lg_burn2), g=10) #not a poor fit

gof(lg_burn)  #AUC=64.0%
gof(lg_burn2) #AUC=66.4%

anova(lg_burn, lg_burn2, test="Chisq")
```


```{r}
table3<- matrix(c(AIC(lg_burn), BIC(lg_burn),
                  AIC(lg_burn2), BIC(lg_burn2)), ncol=2,nrow=2, byrow=TRUE)
colnames(table3)<- c("AIC", "BIC")
rownames(table3)<- c("logistic model #1",
                     "logistic model #2")
table1 <- as.table(table3)
kable(table3)
```



### Gamma Regression for skewed positive distribution 
```{r}
ggplot(data=new_dat, aes(x=log(area+1)))+
  geom_histogram(aes(y=..density..), col="black",fill="white")

new_dat$area_1 <- new_dat$area+1

mod.gam <- glm(area_1~X+Y+month+day+FFMC+DMC+DC+ISI+temp+RH+wind+rain, data=new_dat, family = Gamma(link = "log"))
summary(mod.gam)
test <- gamma.shape(mod.gam, verbose=TRUE)
test
1-pchisq(sum(resid(mod.gam, type="pearson")^2),3.341313)
goft::gamma_test(mod.gam$fitted.values)
```

### Machine Learning
```{r}
y <- new_dat$burn
set.seed(1)
train_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- new_dat[train_index, ]
test_set <- new_dat[-train_index, ]

#Logistic Machine Learning
logistic_fit <- glm(burn~X+Y+month+day+FFMC+I(FFMC^2)+DMC+I(DMC^2)+DC+I(DC^2)+ISI+temp+I(temp^2)+RH+wind+rain+I(rain^2), train_set, family=binomial(link = "logit"))
p_hat_logit <- predict(logistic_fit, newdata=test_set)
y_hat_logit <- ifelse(p_hat_logit>0.5, 1,0)
confusionMatrix(data=as.factor(y_hat_logit), reference = as.factor(test_set$burn))

```




### QDA & LDA
```{r}
set.seed(1)

qda_fit <- qda(burn ~ X+Y+FFMC+DMC+DC+ISI+temp+RH+wind, data = train_set)
qda_preds <- predict(qda_fit, test_set)
confusionMatrix(data=as.factor(qda_preds$class), reference = as.factor(test_set$burn))


lda_fit <- lda(burn ~ X+Y+month+day+FFMC+I(FFMC^2)+DMC+I(DMC^2)+DC+I(DC^2)+ISI+temp+I(temp^2)+RH+wind+rain+I(rain^2), data = train_set)
lda_preds <- predict(lda_fit, test_set)
confusionMatrix(data = as.factor(lda_preds$class), reference = as.factor(test_set$burn))
```

```{r}
library(ROCR)
p1 <- prediction(p_hat_logit, test_set$burn) %>% performance(measure = "tpr", x.measure="tnr")
p2 <- prediction(qda_preds$posterior[,2], test_set$burn) %>% performance(measure = "tpr", x.measure="tnr")
p3 <- prediction(lda_preds$posterior[,2], test_set$burn) %>% performance(measure = "tpr", x.measure="tnr")

par(mfrow=c(1, 2))
p2 %>% plot(main="QDA ROC")
p3 %>% plot(main="LDA ROC")

plot(p1, col="red",xlim=c(1,0))
plot(p2, add=TRUE, col="blue")
plot(p3, add=TRUE, col="green")

# Logistic regression AUC
prediction(p_hat_logit, test_set$burn) %>%
  performance(measure = "auc") %>%
  .@y.values   #0.545


# QDA AUC
prediction(qda_preds$posterior[,2], test_set$burn) %>%
  performance(measure = "auc") %>%
  .@y.values   #0.510

# LDA AUC
prediction(lda_preds$posterior[,2], test_set$burn) %>%
  performance(measure = "auc") %>%
  .@y.values  #0.541
```




## Classification tree, Random Forest
```{r}
library(tree)
library(randomForest)
set.seed(1)
# Classification Tree
train_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- new_dat[train_index, ]
test_set <- new_dat[-train_index, ]
train_set$burn <- as.factor(train_set$burn)
test_set$burn <- as.factor(test_set$burn)

tree_fit <- tree(burn ~X+Y+FFMC+DMC+DC+ISI+temp+RH+wind, data=train_set)
summary(tree_fit)
pred <- predict(tree_fit, test_set,type = "class")
confusionMatrix(as.factor(pred), reference = test_set$burn)

# Random Forest
rf <- randomForest(burn~X+Y+FFMC+DMC+DC+ISI+temp+RH+wind, data = train_set)
preds_rf <- predict(rf, test_set)
confusionMatrix(as.factor(preds_rf), reference = test_set$burn)

# Knn
knn_fit <- knn3(burn~X+Y+FFMC+DMC+DC+ISI+temp+RH+wind, data = train_set,k=5)
knn_hat <- predict(knn_fit, test_set, type = "class")
tab <- table(pred=knn_hat, truth=test_set$burn)
confusionMatrix(tab)

#control <- trainControl(method="cv", number=2,p=0.5)
#dat2 <- mutate(new_dat, burn=as.factor(burn)) 
#res <- train(burn~ X+Y+FFMC+DMC+DC+ISI+temp+RH+wind, data=dat2, method="knn", trControl=control, tuneLength=1, tuneGrid=data.frame(k=seq(1,100,2)), metric="Accuracy")
#plot(res)
```





```{r}
library(data.table)
phat <- predict(logistic_fit, test_set, type="response")
pred <- as.data.frame(test_set[,'burn'], colname='burn')
colnames(pred)[1] <- "burn"
pred$ols <- phat*predict(ols_fit, test_set)
pred$gamma <- phat* predict(gamma_fit, test_set, type="response")

#Simulation
n <- nrow(test_set)
d <- rbind(n, 1, phat)
y.norm <- d*rnorm(n, pred$ols, summary(ols_fit)$sigma)

gamma.shape(gamma_fit, verbose=TRUE)  #a=0.59288122
y.gamma <- d* rgamma(n, shape=0.59288122, rate=0.03498864)
y <- pred$burn
p.dat <- data.table(y=c(y, y.norm, y.gamma), 
                    lab=c(rep("Observed",n), rep("Normal",n), rep("Gamma",n)))
p <- ggplot(p.dat[y>00 & y<400], aes(x=y, col=lab)) +
  geom_density(kernel="gaussian")+
  xlab("area")+ylab("Density")+
  theme(legend.position = 'bottom')+labs(col="")+
  scale_color_manual(values = c(Observed="black", Normal="red", Gamma="blue"))
print(p)
```


























